import os
from datetime import datetime
from langchain_openai import ChatOpenAI
from langchain_core.messages import AIMessage, ToolMessage
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.tools import tool
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_chroma import Chroma
from langchain_community.tools.tavily_search import TavilySearchResults # [æ–°å¢] æœç´¢å·¥å…·

from app.state import AgentState
from app.config import DEEPSEEK_API_KEY, logger 

# --- 1. åˆå§‹åŒ–èµ„æº ---

# RAG: å‘é‡æ•°æ®åº“
DB_PATH = "./chroma_db"
embedding_model = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")

if os.path.exists(DB_PATH):
    vector_store = Chroma(
        persist_directory=DB_PATH, 
        embedding_function=embedding_model
    )
    retriever = vector_store.as_retriever(search_kwargs={"k": 3})
else:
    logger.warning("âš ï¸ æœªæ‰¾åˆ° chroma_db æ–‡ä»¶å¤¹ï¼Œè¯·å…ˆè¿è¡Œ ingest.py")
    retriever = None

# Search: è”ç½‘æœç´¢å·¥å…·
# max_results=3 è¡¨ç¤ºæ¯æ¬¡æœç´¢åªçœ‹å‰3æ¡ç»“æœï¼ŒèŠ‚çœ Token
search_tool = TavilySearchResults(max_results=3)

# --- 2. å®šä¹‰å·¥å…·å‡½æ•° ---

@tool
def get_current_time():
    """
    å½“ç”¨æˆ·è¯¢é—®â€œç°åœ¨å‡ ç‚¹â€ã€â€œä»Šå¤©å‡ å·â€ã€â€œæ˜ŸæœŸå‡ â€æˆ–â€œå½“å‰æ—¶é—´â€æ—¶è°ƒç”¨ã€‚
    è¿”å›ç²¾ç¡®çš„ç³»ç»Ÿæ—¶é—´ã€‚
    """
    return datetime.now().strftime("%Y-%m-%d %H:%M:%S %A")

@tool
def search_knowledge_base(query: str):
    """
    å½“ç”¨æˆ·è¯¢é—®å…³äºâ€œå…¬å¸è§„ç« â€ã€â€œä¸šåŠ¡æ–‡æ¡£â€ã€â€œäº§å“æ‰‹å†Œâ€æˆ–ä»»ä½•å…³äºã€å†…éƒ¨/ç§æœ‰ã€‘çš„å…·ä½“äº‹å®æ—¶è°ƒç”¨ã€‚
    Args:
        query: ç”¨æˆ·çš„æœç´¢å…³é”®è¯
    """
    if not retriever:
        return "é”™è¯¯ï¼šçŸ¥è¯†åº“æœªåˆå§‹åŒ–"
    
    try:
        logger.info(f"ğŸ” [RAG] æ­£åœ¨æ£€ç´¢çŸ¥è¯†åº“: {query}")
        docs = retriever.invoke(query)
        if not docs:
            return "çŸ¥è¯†åº“ä¸­æœªæ‰¾åˆ°ç›¸å…³ä¿¡æ¯ã€‚"
        return "\n\n".join([f"---å†…éƒ¨æ–‡æ¡£ç‰‡æ®µ {i+1}---\n{doc.page_content}" for i, doc in enumerate(docs)])
    except Exception as e:
        return f"æ£€ç´¢å¤±è´¥: {str(e)}"

@tool
def perform_internet_search(query: str):
    """
    å½“ç”¨æˆ·è¯¢é—®ã€å®æ—¶æ–°é—»ã€‘ã€ã€å¤©æ°”ã€‘ã€ã€è‚¡ç¥¨ã€‘ã€ã€å¤–éƒ¨ä¸–ç•Œã€‘çš„é€šç”¨ä¿¡æ¯ï¼Œæˆ–è€…çŸ¥è¯†åº“é‡Œæ²¡æœ‰çš„ä¿¡æ¯æ—¶è°ƒç”¨ã€‚
    ä¾‹å¦‚ï¼šâ€œDeepSeek æœ€æ–°æ¶ˆæ¯â€ã€â€œä»Šå¤©çš„æ–°é—»â€ã€â€œè°æ˜¯ç°åœ¨çš„ç¾å›½æ€»ç»Ÿâ€ã€‚
    Args:
        query: æœç´¢å…³é”®è¯
    """
    try:
        logger.info(f"ğŸŒ [Web] æ­£åœ¨è”ç½‘æœç´¢: {query}")
        # Tavily å¯èƒ½ä¼šæŠ›å‡ºå¼‚å¸¸ï¼Œéœ€è¦æ•è·
        results = search_tool.invoke({"query": query})
        
        # æ ¼å¼åŒ–æœç´¢ç»“æœ
        output = ""
        for res in results:
            output += f"\næ ‡é¢˜: {res.get('content', '')[:100]}...\næ¥æº: {res.get('url')}\n"
        return output
    except Exception as e:
        return f"æœç´¢å¤±è´¥: {str(e)}"

# --- 3. LLM æ ¸å¿ƒèŠ‚ç‚¹ ---

def call_llm_node(state: AgentState):
    if not DEEPSEEK_API_KEY:
        return {"messages": [AIMessage(content="é”™è¯¯ï¼šæœªé…ç½® DeepSeek API Key")]}

    llm = ChatOpenAI(
        model="deepseek-chat",
        openai_api_key=DEEPSEEK_API_KEY,
        openai_api_base="https://api.deepseek.com",
        streaming=True
    )
    
    #  ç»‘å®šä¸‰å¤§å·¥å…·ï¼šæ—¶é—´ + çŸ¥è¯†åº“ + è”ç½‘æœç´¢
    tools = [get_current_time, search_knowledge_base, perform_internet_search]
    llm_with_tools = llm.bind_tools(tools)

    #  è·¯ç”±æç¤ºè¯ (Router Prompt)
    prompt = ChatPromptTemplate.from_messages([
        (
            "system", 
            "ä½ æ˜¯ä¸€ä¸ªå…¨èƒ½å‹çš„ä¼ä¸šæ™ºèƒ½åŠ©æ‰‹ï¼Œæ‹¥æœ‰ä»¥ä¸‹èƒ½åŠ›ï¼š\n"
            "1. `get_current_time`: è·å–å½“å‰æ—¶é—´ã€‚\n"
            "2. `search_knowledge_base`: æŸ¥è¯¢ã€ä¼ä¸šå†…éƒ¨ã€‘ç§æœ‰æ–‡æ¡£ã€‚\n"
            "3. `perform_internet_search`: é€šè¿‡æœç´¢å¼•æ“æŸ¥è¯¢ã€å¤–éƒ¨äº’è”ç½‘ã€‘çš„å®æ—¶ä¿¡æ¯ã€‚\n\n"
            "å†³ç­–é€»è¾‘ï¼š\n"
            "- é—®æ—¶é—´ -> æŸ¥æ—¶é—´ã€‚\n"
            "- é—®å…¬å¸å†…éƒ¨äº‹åŠ¡ï¼ˆå¦‚ä¸Šç­æ—¶é—´ã€æŠ¥é”€æµç¨‹ï¼‰ -> æŸ¥çŸ¥è¯†åº“ã€‚\n"
            "- é—®å¤–éƒ¨æ–°é—»ã€å¤©æ°”ã€è‚¡ç¥¨ã€æˆ–é€šç”¨çŸ¥è¯† -> è”ç½‘æœç´¢ã€‚\n"
            "- é—²èŠ -> ç›´æ¥å›å¤ã€‚"
        ),
        MessagesPlaceholder(variable_name="messages"),
    ])

    chain = prompt | llm_with_tools

    try:
        result = chain.invoke({"messages": state["messages"]})
        return {"messages": [result]}

    except Exception as e:
        logger.error(f"LLM è°ƒç”¨å¤±è´¥: {e}")
        return {"messages": [AIMessage(content=f"ç³»ç»Ÿé”™è¯¯: {str(e)}")]}

# --- 4. å·¥å…·æ‰§è¡ŒèŠ‚ç‚¹ ---

def tool_node(state: AgentState):
    messages = state["messages"]
    last_message = messages[-1]
    
    tool_outputs = []
    
    if hasattr(last_message, "tool_calls"):
        for tool_call in last_message.tool_calls:
            tool_name = tool_call["name"]
            tool_args = tool_call["args"]
            
            logger.info(f"âš™ï¸ æ‰§è¡Œå·¥å…·: {tool_name}")
            
            # å·¥å…·è·¯ç”±åˆ†å‘
            if tool_name == "get_current_time":
                output = get_current_time.invoke(tool_args)
            elif tool_name == "search_knowledge_base":
                output = search_knowledge_base.invoke(tool_args)
            elif tool_name == "perform_internet_search":
                output = perform_internet_search.invoke(tool_args)
            else:
                output = "æœªçŸ¥å·¥å…·"

            tool_outputs.append(ToolMessage(
                content=str(output),
                tool_call_id=tool_call["id"],
                name=tool_name
            ))
    
    return {"messages": tool_outputs}